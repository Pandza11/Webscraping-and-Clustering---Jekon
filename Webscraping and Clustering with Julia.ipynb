{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping and Clustering with Juno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is a high-level, high-performance dynamic programming language for technical computing, with syntax that is familiar to users of other technical computing environments. It provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.\n",
    "\n",
    "In this notebook, we will show how we used the programming language Julia in order to:\n",
    "1. Parse text from a website\n",
    "2. Perform text clustering, an unsupervised machine learning technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installing Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to install Julia on your computer, visit http://julialang.org/downloads/ and follow instructions.\n",
    "\n",
    "You can use Julia online, in a browser. JuliaBox provides online IJulia notebooks, which let you run Julia on a remote machine, using Jupyter (formerly called IPython) interactive notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Webscraping with Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. We will try to use several web scraping libraries or modules, some of them written for the Julia language, and some of them import from other programming languages like Python.\n",
    "\n",
    "In order for us to be able to use a Python library, we need to type in the Julia console Pkg.add(\"PyCall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.1 Requests.jl\n",
    "\n",
    "Requests.jl is an HTTP client written in Julia. Read more about it at https://github.com/JuliaWeb/Requests.jl.\n",
    "\n",
    "In order for us to be able to use a Python library, we need to type in the Julia console Pkg.add(\"Requests\")\n",
    "\n",
    "Unfortunately, the library does not seem to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: Requests not found in path\nwhile loading In[19], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: Requests not found in path\nwhile loading In[19], in expression starting on line 1",
      "",
      " in require at loading.jl:249"
     ]
    }
   ],
   "source": [
    "using Requests\n",
    "url = \"http://lavica.fesb.unist.hr/mat1/\"\n",
    "firstrequest = get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.2 HTTPClient.jl \n",
    "\n",
    "Provides HTTP client functionality based on libcurl. Seems it has been deprecated in favour of Requests.jl:\n",
    "\n",
    "https://github.com/JuliaWeb/HTTPClient.jl/issues/21\n",
    "\n",
    "In order for us to be able to use a Python library, we need to type in the Julia console Pkg.add(\"HTTPClient\"), Pkg.add(\"URIParser\") and Pkg.add(\"Gumbo\").\n",
    "\n",
    "After trying to run the module, several problems arise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "parsePage (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base.String is deprecated, use AbstractString instead.\n",
      "  likely near In[8]:13\n",
      "WARNING: Base.String is deprecated, use AbstractString instead.\n",
      "  likely near In[8]:13\n",
      "WARNING: Base.String is deprecated, use AbstractString instead.\n",
      "  likely near In[8]:13\n"
     ]
    }
   ],
   "source": [
    "using HTTPClient.HTTPC\n",
    "using URIParser\n",
    "using Gumbo\n",
    " \n",
    "#callback for HTTPC.get, allow to use libCURL options\n",
    "function customize_curl(curl)\n",
    "  cc = LibCURL.curl_easy_setopt(curl, LibCURL.CURLOPT_USERAGENT, \"Mozilla/5.0 (Windows NT 6.1; rv:28.0) Gecko/20100101 Firefox/28.0\")\n",
    "  if cc != LibCURL.CURLE_OK\n",
    "    error (\"CURLOPT_USERAGENT failed: \" * LibCURL.bytestring(curl_easy_strerror(cc)))\n",
    "  end\n",
    "end\n",
    " \n",
    "function getPage(url::String;debug=true)\n",
    " \n",
    "    try\n",
    " \n",
    "        r = HTTPC.get(url,RequestOptions(\n",
    "                        request_timeout=8.0,\n",
    "                        callback=customize_curl\n",
    "                    ))\n",
    " \n",
    "        if r.http_code != 200\n",
    "            code = r.http_code\n",
    "            if debug\n",
    "                warn(\"couldn't read url : $url, HTTP code : $code\")\n",
    "            end\n",
    "            return (false,\"\")\n",
    "        end\n",
    " \n",
    "        page = bytestring(r.body)\n",
    " \n",
    "        return (true, page)\n",
    " \n",
    "    catch err\n",
    " \n",
    "        if debug\n",
    "            println(err)\n",
    "        end\n",
    "        return (false,\"\")\n",
    "    end\n",
    "end\n",
    " \n",
    "function getBody(doc)\n",
    " \n",
    "    body = HTMLElement(:body)\n",
    "    for elem in preorder(doc.root)\n",
    "        if typeof(elem) == HTMLElement{:body}\n",
    "            body = elem\n",
    "            break\n",
    "        end\n",
    "    end\n",
    " \n",
    "    return body\n",
    " \n",
    "end\n",
    " \n",
    "function parsePage(page)\n",
    " \n",
    "    doc = parsehtml(page)\n",
    "    body = getBody(doc)\n",
    " \n",
    "    postUrls = String[]\n",
    "    titles = String[]\n",
    "    nextUrl = \"\"\n",
    " \n",
    "    for elem in preorder(body)\n",
    "        if typeof(elem) == HTMLElement{:a}\n",
    " \n",
    "            try\n",
    "                as = attrs(elem)\n",
    "                if haskey(as,\"class\") && as[\"class\"] == \"title may-blank \"\n",
    "                    push!(titles, lowercase(elem[1].text) )\n",
    "                    push!(postUrls,  as[\"href\"])\n",
    "                end\n",
    " \n",
    "                if haskey(as,\"rel\") && as[\"rel\"] == \"nofollow next\"\n",
    " \n",
    "                    nextUrl = as[\"href\"]\n",
    "                end\n",
    "            end\n",
    " \n",
    "        end\n",
    "    end\n",
    " \n",
    "    return postUrls, titles, nextUrl\n",
    " \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.3 Urllib2\n",
    "\n",
    "We will try using a Python web scraping module in Julia. In order to do so, we first need to add the PyCall package in the Julia console: Pkg.add(\"PyCall\")\n",
    "\n",
    "The urllib2 module defines functions and classes which help in opening URLs (mostly HTTP).\n",
    "\n",
    "Using urllib2 we can easily access the content of an url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Recompiling stale cache file C:\\Users\\Ja\\.julia\\lib\\v0.4\\PyCall.ji for module PyCall.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD HTML 4.0 Transitional//EN\\\">\\n<html>\\n<head>\\n<title> Matematika 1 </title> \\n<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=UTF-8\\\">\\n<link rel=\\\"shortcut icon\\\" href=\\\"favicon.ico\\\"/>\\n</HEAD>\\n<frameset rows=\\\"46,50,*\\\" border=0>\\n<frame src=\\\"banner.html\\\" name=\\\"Banner\\\" scrolling=\\\"no\\\">\\n<frame src=\\\"f_mat1.html\\\" name=\\\"Upravljanje\\\" scrolling=\\\"no\\\">\\n<frame src=\\\"osnovna.html\\\" name=\\\"Tekst\\\">\\n</frameset>\\n</html>\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "@pyimport urllib2 as urllib2\n",
    "req = pycall(urllib2.Request, PyAny, \"http://lavica.fesb.unist.hr/mat1/\")\n",
    "html_doc=urllib2.urlopen(req)\n",
    "html_doc[:read](500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#1.4 BeautifulSoup\n",
    "\n",
    "Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping. Three features make it powerful:\n",
    "\n",
    "- Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree: a toolkit for dissecting a document and extracting what you need. It doesn't take much code to write an application\n",
    "- Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8. You don't have to think about encodings, unless the document doesn't specify an encoding and Beautiful Soup can't detect one. Then you just have to specify the original encoding.\n",
    "- Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility. \n",
    "\n",
    "The module does not seem to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: syntax: invalid character literal\nwhile loading In[8], in expression starting on line 4",
     "output_type": "error",
     "traceback": [
      "LoadError: syntax: invalid character literal\nwhile loading In[8], in expression starting on line 4",
      ""
     ]
    }
   ],
   "source": [
    "@pyimport bs4 \n",
    "@pyimport BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.5 LibCURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl url exec response : 0\n",
      "httpcode : Int32[200]\n"
     ]
    }
   ],
   "source": [
    "# Kod s Githuba\n",
    "using LibCURL\n",
    "# init a curl handle\n",
    "curl = curl_easy_init()\n",
    "\n",
    "# set the URL and request to follow redirects\n",
    "curl_easy_setopt(curl, CURLOPT_URL, \"http://example.com\")\n",
    "curl_easy_setopt(curl, CURLOPT_FOLLOWLOCATION, 1)\n",
    "# setup the callback function to recv data\n",
    "function curl_read_cb(curlbuf::Ptr{Void}, s::Csize_t, n::Csize_t, p_ctxt::Ptr{Void})\n",
    "    sz = s * n\n",
    "    data = Array(UInt8, sz)\n",
    "\n",
    "    ccall(:memcpy, Ptr{Void}, (Ptr{Void}, Ptr{Void}, UInt64), curlbuf, data, sz)\n",
    "    println(\"recd: \", bytestring(data))\n",
    "\n",
    "    sz::Csize_t\n",
    "end\n",
    "\n",
    "c_curl_read_cb = cfunction(curl_read_cb, Csize_t, (Ptr{Void}, Csize_t, Csize_t, Ptr{Void}))\n",
    "curl_easy_setopt(curl, CURLOPT_READFUNCTION, c_curl_read_cb)\n",
    "\n",
    "\n",
    "# execute the query\n",
    "res = curl_easy_perform(curl)\n",
    "println(\"curl url exec response : \", res)\n",
    "\n",
    "# retrieve HTTP code\n",
    "http_code = Array(Clong,1)\n",
    "curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, http_code)\n",
    "println(\"httpcode : \", http_code)\n",
    "\n",
    "# release handle\n",
    "curl_easy_cleanup(curl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: imported binding for CURLOPT_URL overwritten in module Main\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: error compiling anonymous: could not load library \"libcurl\"\nThe specified module could not be found.\r\n\nwhile loading In[10], in expression starting on line 5",
     "output_type": "error",
     "traceback": [
      "LoadError: error compiling anonymous: could not load library \"libcurl\"\nThe specified module could not be found.\r\n\nwhile loading In[10], in expression starting on line 5",
      ""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: imported binding for CURLOPT_FOLLOWLOCATION overwritten in module Main\n",
      "WARNING: Base.Uint8 is deprecated, use UInt8 instead.\n",
      "  likely near In[10]:5\n"
     ]
    }
   ],
   "source": [
    "# Kod iz knjige Mastering Julia\n",
    "const CURLOPT_URL = 10002\n",
    "const CURLOPT_FOLLOWLOCATION = 52;\n",
    "const CURLE_OK = 0\n",
    "jlo = \"http://julialang.org\";\n",
    "curl = ccall( (:curl_easy_init, \"libcurl\"), Ptr{Uint8}, ())\n",
    "ccall((:curl_easy_setopt, \"libcurl\"), Ptr{Uint8},\n",
    "(Ptr{Uint8}, Int, Ptr{Uint8}), curl, CURLOPT_URL, jlo.data)\n",
    "ccall((:curl_easy_perform,\"libcurl\"),\n",
    "Ptr{Uint8}, (Ptr{Uint8},), curl)\n",
    "ccall((:curl_easy_cleanup,\"libcurl\"),\n",
    "Ptr{Uint8},(Ptr{Uint8},), curl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.6 Parsing\n",
    "\n",
    "Since we did not manage to find a module which allows for efficient parsing of html documents, we can use the replace function and regular expressions in order to remove the html tags from the html document we acquired in 1.3 Urllib2 (PyObject f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"PyObject <addinfourl at 418455048L whose fp = <socket._fileobject object at 0x0000000018E9C9A8>>\"\n"
     ]
    }
   ],
   "source": [
    "#Funkcijom replace (na primitivniji način) mogli bismo se riješiti html tagova\n",
    "s1 = \"The quick brown brown fox jumps over the lazy dog α,β,γ\"\n",
    "r = replace(string(f), \"menu\", \"menu1\")\n",
    "show(r); println()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since we acquired the html document using a Python module, it is currently stored as a PyObject. In order to convert it, we need to use the following instructions:\n",
    "\n",
    "https://github.com/stevengj/PyCall.jl\n",
    "\n",
    "or:\n",
    "\n",
    "http://stackoverflow.com/questions/5356773/python-get-string-representation-of-pyobject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: T not defined\nwhile loading In[17], in expression starting on line 2",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: T not defined\nwhile loading In[17], in expression starting on line 2",
      ""
     ]
    }
   ],
   "source": [
    "f1 = convert(T, o::f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "We were not able to convert the PyObject into a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Clustering\n",
    "\n",
    "Cluster analysis or clustering, a type of unsupervised machine learning, is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).\n",
    "\n",
    "Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them.\n",
    "\n",
    "In order to perform clustering in Julia, we could use Clustering.jl:\n",
    "\n",
    "https://github.com/JuliaStats/Clustering.jl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
